#include <torch/csrc/jit/graph_executor.h>
#include <torch/csrc/jit/jit_log.h>
#include <torch/csrc/jit/passes/specialize_autogradzero.h>

namespace torch {
namespace jit {

// propagate autograd zero information through a gradient graph and
// remove grad_of blocks if present.
// Note: this is a very limited pass. It only propagates autograd zeros for
// operations generated by the symbolic autodiff code and cleans up
// AutogradAdds when possible. Outputs of other nodes are conservatively
// marked Unknown and not optimized.
void specializeAutogradZero(Graph &g) {
  enum class State { Nonzero, Zero, Unknown };
  std::unordered_map<Value*, State> state;

  for (Value* input : g.inputs()) {
    const auto& tp = input->type();
    if (auto tt = tp->cast<TensorType>()) {
      if (tt->undefined() && *tt->undefined()) {
        state[input] = State::Zero;
      } else {
        state[input] = State::Nonzero;
      }
    } else if (
        tp->isSubtypeOf(TensorType::get()) ||
        tp->isSubtypeOf(ListType::ofTensors())) {
      state[input] = State::Nonzero;
    } else {
      state[input] = State::Unknown;
    }
  }

  for (auto it = g.nodes().begin(); it != g.nodes().end(); ++it) {
    auto n = *it;
    switch (n->kind()) {
      case prim::GradOf: {
        auto all_zeros =
            std::all_of(n->inputs().begin(), n->inputs().end(), [&](Value* v) {
              return state[v] == State::Zero;
            });
        // Property 1: if all the gradInputs to the GradOf are Zero
        // then the gradOutputs are also zero and will be represented as
        // AutogradZero nodes
        if (all_zeros) {
          auto zero = g.createAutogradZero()->insertAfter(n)->output();
          for (auto o : n->outputs()) {
            GRAPH_UPDATE("Replacing output %", o->debugName(),
                         " with AutogradZero %", zero->debugName());
            o->replaceAllUsesWith(zero);
          }
        } else {
          // Property 2: GradOfs are required to correctly handle combinations
          // of Nonzero and zero inputs. They are expected to produce
          // Nonzero output tensors in this case.

          // Remove the GradOf, splicing its body back into the surrounding
          // block
          auto body = n->blocks().at(0);
          for (auto input : n->inputs()) {
            // we should never get into a situation when specializing a GradOf
            // where we do not know if a value is Nonzero since at the top level
            // a gradient graph is composed of Linear nodes and AutogradAdds
            // and LinearNodes only appear in these graphs
            AT_ASSERT(state[input] != State::Unknown);
          }
          // hoist the nodes in the GradOf body to be before the linear block
          GRAPH_UPDATE("Hoisting out ", getHeader(*it));
          for (auto it = body->nodes().begin(); it != body->nodes().end();) {
            auto block_node = *it++;
            block_node->moveBefore(n);
          }

          for (size_t i = 0; i < n->outputs().size(); ++i) {
            GRAPH_UPDATE("Replacing prim::GradOf's use %",
                         n->outputs().at(i)->debugName(),
                         " with hoisted value %",
                         body->outputs().at(i)->debugName());
            n->outputs().at(i)->replaceAllUsesWith(body->outputs().at(i));
          }
        }
        GRAPH_UPDATE("Destroying ", getHeader(*it));
        it.destroyCurrent();
      } break;
      case prim::AutogradAdd: {
        auto a = n->input(0);
        auto b = n->input(1);
        // if one is Autograd zero, we can just drop the add
        if (state[a] == State::Zero) {
          // Zero + b == b
          GRAPH_UPDATE("Simplifying ", getHeader(n), " where %", a->debugName(),
                       " is AutogradZero to %", b->debugName());
          n->output()->replaceAllUsesWith(b);
          it.destroyCurrent();
        } else if (state[b] == State::Zero) {
          // a + Zero == a
          GRAPH_UPDATE("Simplifying ", getHeader(n), " where %", b->debugName(),
                       " is AutogradZero to %", a->debugName());
          n->output()->replaceAllUsesWith(a);
          it.destroyCurrent();
        } else if (state[a] == State::Nonzero && state[b] == State::Nonzero) {
          // when both are Nonzero, we can use a normal, optimizable add
          // instruction

          WithInsertPoint guard(n);
          auto* g = n->owningGraph();
          auto* cOne = g->insertConstant(1);
          auto* add_node = g->insertNode(g->create(aten::add, 1));
          add_node->addInput(a);
          add_node->addInput(b);
          add_node->addInput(cOne);
          auto* add_output = add_node->output();
          state[add_output] = State::Nonzero;
          n->output()->replaceAllUsesWith(add_output);
          GRAPH_UPDATE("Simplifying ", getHeader(n), " to ",
                       getHeader(add_node));
          it.destroyCurrent();
        } else {
          // otherwise we have conditionally-Nonzero things, and we need
          // to actually run an AutogradAdd which will guard for Zeros
          // so we leave the op as is
          state[n->output()] = State::Unknown;
        }
      } break;
      case prim::AutogradZero: {
        state[n->output()] = State::Zero;
      } break;
      case prim::profile: {
        // if prim::profile doesn't have an input
        // it's a counter to keep track how many times
        // a graph was profiled
        if (n->inputs().size() > 0) {
          state[n->output()] = State::Unknown;
          // state[n->input()];
        }
        break;
      }
      case prim::BailOut: {
        if (auto ptt = n->output()->type()->expect<TensorType>()) {
          state[n->output()] =
              ptt->undefined()
                  ? *ptt->undefined() ? State::Zero : State::Nonzero
                  : State::Unknown;
        }
      } break;
      case prim::Guard: {
        if (auto ptt = n->output()->type()->expect<TensorType>()) {
          state[n->output()] =
              ptt->undefined()
                  ? *ptt->undefined() ? State::Zero : State::Nonzero
                  : State::Unknown;
        }
      } break;
      default:
        for (auto o : n->outputs()) {
          state[o] = State::Unknown;
        }
        break;
    }
  }
}

} // namespace jit
} // namespace torch
